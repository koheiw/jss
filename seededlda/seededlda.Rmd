---
documentclass: jss
author:
  - name: Kohei Watanabe
    orcid: 0000-0001-6519-5265
    affiliation: Waseda University
    email: \email{watanabe.kohei@gmail.com}
    url: https://koheiw.net
title:
  formatted: "Distributed Asymmetric Allocation of Topics for Large Imbalanced Corpora in \\proglang{R}"
  # If you use tex in the formatted title, also supply version without
  plain: "Distributed Asymmetric Allocation of Topics for Large Imbalanced Corpora in R"
  # For running headers, if needed
  short: "Distributed Asymmetric Allocation"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
header-includes:
   - \usepackage{algorithm}
   - \usepackage[noend]{algpseudocode}
output: rticles::jss_article
---

```{r, setup, include=FALSE}
# NOTE: run rmarkdown::render("seededlda/seededlda.Rmd") in WSL to generate PDF
options(prompt = 'R> ', continue = '+ ', fig.align = 'center')
```

# Introduction

## Algorithms

To infer the document-topic distribution $\theta$ and the topic-word distribution $\phi$, assigned topics are saved in $M_{dk}$ and $N_{kv}$. The former is the frequency of topic k found in document $d$, the latter is the frequency of topic $k$ assigned to unique word $v$, and $\alpha_k$ and $\beta_k$ are the Dirichlet priors, which are added to the frequency counts to smooth the distributions in $\theta$ and $\phi$.  The Gibbs sampler assigns topics to the words in the corpus based on the sampling distribution, $G$, derived as a product of $\theta$ and $\phi$. These relationships between are defined as follows:

$$
G = P(Z=k|W=v,d)\propto\theta_{dk}\phi_{kv}
$$
$$
\theta_{dk} = P(Z=k|d,\alpha_k) = \frac{M_{dk}+\alpha_k}{M_{d.}+\sum\alpha_k}
$$
$$
\phi_{kv} = P(W=v|k,\beta_k) = \frac{N_{kv}+\beta_k}{N_{k.}+\sum\beta_k}
$$

![
Graphical model of a simple LDA. Gray circles are variables whose values are known, 
whereas white circles are latent variables whose values are unknown. $\theta$ and $\phi$ 
are the $|D| \times K$ and $K \times |V|$ matrices, respectively; $Z$ is a vector to record 
topics words, $W$, in document $d$; $d$ is one of the documents $d \in D$; $v$ is one of the 
unique words $v \in V$ in the corpus.
]("../figures/lda.png")


\makeatletter
\renewcommand{\ALG@name}{Algorithm}
\makeatother
\begin{algorithm}
\caption{Pseudo-code for a simple LDA. Gibbs sampling is repeated max\_iter times.}
\begin{algorithmic}

\State \textbf{initialize} randomly assign topics to $Z$
\For {$i \gets 1$ to $max\_iter$}
  \State sample topic for words: $z_i \gets G(N_{kv}, M_{dk}, \alpha_k, \beta_k)$
\EndFor

\end{algorithmic}
\end{algorithm}

-------------------

\makeatletter
\renewcommand{\ALG@name}{Algorithm}
\makeatother
\begin{algorithm}
\caption{
Pseudo-code for the enhanced LDA with distributed computing. Gibbs sampling is repeated max\_iter times in separate processors over $D \times batch\_size$ documents; $D$ is the total number of documents in the corpus.
}
\begin{algorithmic}

\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}

\State \textbf{initialize} randomly assign topics to $Z$
\For {$i \gets 1$ to $max\_iter/10$}
  \State	assign $D \times batch\_size$ documents to processor $e$
  \State \textbf{parallel\_for} {$j \gets 1$ to $10$} \textbf{do}
  \Indent
    \State sample topic for words: $z_i \gets G(N_{kv}, M_{dk}, \alpha_k, \beta_k)$
    \State \Return ${\acute{N}}_{kv}^e$ 
  \EndIndent
  \State synchronize topic-word count: $N_{kv} \gets N_{kv}+{\acute{N}}_{kv}^1+{\acute{N}}_{kv}^2+\cdots{\acute{N}}_{kv}^e$
\EndFor
\end{algorithmic}
\end{algorithm}




