---
documentclass: jss
author:
  - name: Kohei Watanabe
    orcid: 0000-0001-6519-5265
    affiliation: Waseda University
    email: \email{watanabe.kohei@gmail.com}
    url: https://posit.co
    # use a different affiliation in adress field (differently formated here)
    affiliation2: Universitat AutÃ²noma de Barcelona
title:
  formatted: "Distributed Asymmetric Allocation of Topics for Large Imbalanced Corpora in \\proglang{R}"
  # If you use tex in the formatted title, also supply version without
  plain: "Distributed Asymmetric Allocation of Topics for Large Imbalanced Corpora in R"
  # For running headers, if needed
  short: "Distributed Asymmetric Allocation"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
header-includes:
   - \usepackage{algorithm}
   - \usepackage[noend]{algpseudocode}
output: rticles::jss_article
---

```{r, setup, include=FALSE}
# NOTE: run rmarkdown::render("seededlda/seededlda.Rmd") in WSL to generate PDF
options(prompt = 'R> ', continue = '+ ', fig.align = 'center')
```

# Introduction

## Algorithms

To infer the document-topic distribution $\theta$ and the topic-word distribution $\phi$, assigned topics are saved in $M_{dk}$ and $N_{kv}$. The former is the frequency of topic k found in document $d$, the latter is the frequency of topic $k$ assigned to unique word $v$, and $\alpha_k$ and $\beta_k$ are the Dirichlet priors, which are added to the frequency counts to smooth the distributions in $\theta$ and $\phi$.  The Gibbs sampler assigns topics to the words in the corpus based on the sampling distribution, $G$, derived as a product of $\theta$ and $\phi$. These relationships between are defined as follows:

$$
\begin{split}
G = P(Z=k|W=v,d)\propto\theta_{dk}\phi_{kv} \\
\theta_{dk} = P(Z=k|d,\alpha_k) = \frac{M_{dk}+\alpha_k}{M_{d.}+\sum\alpha_k} \\
\phi_{kv} = P(W=v|k,\beta_k) = \frac{N_{kv}+\beta_k}{N_{k.}+\sum\beta_k}
\end{split}
$$

![
Graphical model of a simple LDA. Gray circles are variables whose values are known, 
whereas white circles are latent variables whose values are unknown. $\theta$ and $\phi$ 
are the $|D| \times K$ and $K \times |V|$ matrices, respectively; $Z$ is a vector to record 
topics words, $W$, in document $d$; $d$ is one of the documents $d \in D$; $v$ is one of the 
unique words $v \in V$ in the corpus.
]("../figures/lda.png")


\makeatletter
\renewcommand{\ALG@name}{Algorithm}
\makeatother
\begin{algorithm}[tb]
\caption{Pseudo-code for a simple LDA. Gibbs sampling is repeated max\_iter times.}
\begin{algorithmic}[1]

\Function {max\_in\_array}{$array$}
    \State $max \gets 0$
    \ForAll {$element \gets array$}
        \If {$element > max$}
            \State $max \gets element$
        \EndIf
    \EndFor
    \State \Return $max$
\EndFunction

\end{algorithmic}
\end{algorithm}


