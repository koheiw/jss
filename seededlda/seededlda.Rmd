---
documentclass: jss
author:
  - name: Kohei Watanabe
    orcid: 0000-0001-6519-5265
    affiliation: Waseda University
    email: \email{watanabe.kohei@gmail.com}
    url: https://posit.co
    # use a different affiliation in adress field (differently formated here)
    affiliation2: Universitat Autònoma de Barcelona
title:
  formatted: "Distributed Asymmetric Allocation of Topics for Large Imbalanced Corpora in \\proglang{R}"
  # If you use tex in the formatted title, also supply version without
  plain: "Distributed Asymmetric Allocation of Topics for Large Imbalanced Corpora in R"
  # For running headers, if needed
  short: "Distributed Asymmetric Allocation"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
output: rticles::jss_article
---

```{r, setup, include=FALSE}
# NOTE: run rmarkdown::render("seededlda/seededlda.Rmd") in WSL to generate PDF
options(prompt = 'R> ', continue = '+ ', fig.align = 'center')
```


![
Graphical model of a simple LDA. Gray circles are variables whose values are known, 
whereas white circles are latent variables whose values are unknown. $\theta$ and $\phi$ 
are the $|D| \times K$ and $K \times |V|$ matrices, respectively; $Z$ is a vector to record 
topics words, $W$, in document $d$; $d$ is one of the documents $d \in D$; $v$ is one of the 
unique words $v \in V$ in the corpus.
]("../figures/lda.png")

Can be inserted in regular R markdown blocks.

```{r}
x <- 1:10
x
```

To infer the document-topic distribution $\theta$ and the topic-word distribution $\phi$, assigned topics are saved in $M_{dk}$ and $N_{kv}$. The former is the frequency of topic k found in document $d$, the latter is the frequency of topic $k$ assigned to unique word $v$, and $α_k$ and $β_k$ are the Dirichlet priors, which are added to the frequency counts to smooth the distributions in $\theta$ and $\phi$.  The Gibbs sampler assigns topics to the words in the corpus based on the sampling distribution, $G$, derived as a product of $\theta$ and $\phi$. These relationships between are defined as follows:

$$
G = P(Z=k|W=v,d)\propto\theta_{dk}\phi_{kv} \\
\theta_{dk} = P(Z=k|d,\alpha_k) = \frac{M_{dk}+\alpha_k}{M_{d.}+\sum\alpha_k}	\\
\phi_{kv} = P(W=v|k,\beta_k) = \frac{N_{kv}+\beta_k}{N_{k.}+\sum\beta_k}
$$


